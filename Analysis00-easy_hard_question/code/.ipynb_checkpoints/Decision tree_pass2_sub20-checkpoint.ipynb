{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Import data set and get X and y matrix for test & train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    make_scorer\n",
    ")\n",
    "from imblearn.metrics import specificity_score\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "import boto3\n",
    "from sklearn.model_selection import KFold, LeaveOneOut, cross_val_score\n",
    "\n",
    "# import data\n",
    "df = pd.read_csv(\"../data/pass2_sub20.csv\")\n",
    "filtering = \"pass2\"\n",
    "sample = \"20\"\n",
    "# Keep only the columns with certain names\n",
    "columns_to_keep = ['Median', 'question_type']\n",
    "feature_selected = ','.join(columns_to_keep[:-1])\n",
    "#columns_to_keep = ['Mean', 'Median', 'Mode', 'SD', 'direction_pct','complete_time_median', 'complete_time_sd', 'question_type']\n",
    "df2 = df.loc[:, columns_to_keep]\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df2.drop('question_type', axis=1)  # Features (all columns except \"question_type\")\n",
    "y = df2['question_type']               # Target variable (\"question_type\")\n",
    "\n",
    "# Create a new DataFrame to store the percentage of \"hard\" and \"easy\" rows\n",
    "y_counts = pd.DataFrame(y.value_counts(normalize=True))\n",
    "y_counts.columns = ['Percentage']\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "example_no = X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start running MLflow to keep track of all the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ActiveRun: >"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are to connect to the AWS MLflow server\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = 'AKIAWR3SYCDRCO6I3XGK'\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = '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'\n",
    "\n",
    "\n",
    "# set registry URI i.e. where MLflow saves runs\n",
    "# mlflow.set_tracking_uri(\"file:///Users/ahmed.besbes/projects/mlflow/\")\n",
    "mlflow.set_tracking_uri(\"http://ec2-35-92-91-170.us-west-2.compute.amazonaws.com:5000\")\n",
    "\n",
    "# experiment_id = mlflow.create_experiment(\"training experiment\")\n",
    "experiment_id = mlflow.set_experiment(\"amazon_cv\").experiment_id\n",
    "\n",
    "# Initialize an MLflow experiment\n",
    "mlflow.start_run(experiment_id=experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 score: 0.668888888888889\n",
      "Average accuracy score: 0.681818181818182\n",
      "Average sensitivity score: 0.6388888888888888\n",
      "Average specificity score: 0.7230769230769231\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "depth = 3\n",
    "clf = DecisionTreeClassifier(random_state=42, max_depth = depth)\n",
    "pos_label = 'hard'  # set the positive class label based on your data\n",
    "\n",
    "# Define the cross-validation method\n",
    "cv = KFold(n_splits=3)\n",
    "\n",
    "# Evaluate the model using LOOCV\n",
    "accuracy = cross_val_score(clf, X, y, cv=cv, scoring=make_scorer(accuracy_score))\n",
    "f1 = cross_val_score(clf, X, y, cv=cv, scoring=make_scorer(f1_score, pos_label=pos_label))\n",
    "sensitivity = cross_val_score(clf, X, y, cv=cv, scoring= make_scorer(recall_score, pos_label=pos_label))\n",
    "specificity = cross_val_score(clf, X, y, cv=cv, scoring= make_scorer(specificity_score, pos_label=pos_label))\n",
    "\n",
    "# Print the scores\n",
    "#print('F1 scores:', scores[0])\n",
    "#print('Accuracy scores:', accuracy)\n",
    "#print('Sensitivity scores:', scores[2])\n",
    "#print('Specificity scores:', scores[3])\n",
    "print('Average F1 score:', np.mean(f1))\n",
    "print('Average accuracy score:', np.mean(accuracy))\n",
    "print('Average sensitivity score:', np.mean(sensitivity))\n",
    "print('Average specificity score:', np.mean(specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy score: 0.7575757575757576\n"
     ]
    }
   ],
   "source": [
    "# leave-one-out cross validation\n",
    "# Define the model\n",
    "clf = DecisionTreeClassifier(random_state=42, max_depth = depth)\n",
    "pos_label = 'hard'  # set the positive class label based on your data\n",
    "\n",
    "# Define the cross-validation method\n",
    "cv = LeaveOneOut()\n",
    "\n",
    "# Evaluate the model using LOOCV\n",
    "accuracy_loo = cross_val_score(clf, X, y, cv=cv, scoring=make_scorer(accuracy_score))\n",
    "\n",
    "# Print the scores\n",
    "print('Average accuracy score:', np.mean(accuracy_loo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log metrics\n",
    "metrics = {\"test_accuracy\": np.mean(accuracy), \"LOOCV_accuracy\": np.mean(accuracy_loo),\"sensitivity\": np.mean(sensitivity), \"specificity\": np.mean(specificity), \"f1\": np.mean(f1)}\n",
    "        \n",
    "# log dataframe used\n",
    "params = {\"Model type\": \"decision_tree\", \"Max depth\": depth, \"filtering\": filtering, \"sampling\": sample, \"features selected\": feature_selected, \"example number\": example_no}\n",
    "        \n",
    "# enable autologging\n",
    "# mlflow.sklearn.autolog()\n",
    "mlflow.log_metrics(metrics)\n",
    "mlflow.log_params(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- direction_pct <= 52.50\n",
      "|   |--- class: hard\n",
      "|--- direction_pct >  52.50\n",
      "|   |--- class: easy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import export_text\n",
    "\n",
    "# Print out the decision tree structure\n",
    "tree_rules = export_text(clf, feature_names=list(X.columns))\n",
    "print(tree_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# backup code not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/03/19 16:30:02 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during sklearn autologging: Failed to upload /var/folders/tt/5v8l17gn0fdbw73gwsn54j180000gn/T/tmpfurlmpb3/training_roc_curve.png to mlflow-artifact-store-demo/9/13a508413d524758b0beed3a3ac6276f/artifacts/training_roc_curve.png: An error occurred (AccessDenied) when calling the PutObject operation: Access Denied\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        easy       0.75      0.92      0.83        26\n",
      "        hard       0.86      0.60      0.71        20\n",
      "\n",
      "    accuracy                           0.78        46\n",
      "   macro avg       0.80      0.76      0.77        46\n",
      "weighted avg       0.80      0.78      0.77        46\n",
      "\n",
      "\n",
      "Test report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        easy       0.50      1.00      0.67         7\n",
      "        hard       1.00      0.46      0.63        13\n",
      "\n",
      "    accuracy                           0.65        20\n",
      "   macro avg       0.75      0.73      0.65        20\n",
      "weighted avg       0.82      0.65      0.64        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train a decision tree model\n",
    "clf = DecisionTreeClassifier(random_state=42, max_depth = 1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the class labels for the training set\n",
    "y_pred_train = clf.predict(X_train)\n",
    "\n",
    "# Compute the classification report for the training set\n",
    "report_train = classification_report(y_train, y_pred_train)\n",
    "\n",
    "# Print the classification report for the training set\n",
    "print(\"Training report:\\n\", report_train)\n",
    "\n",
    "# Predict the class labels for the test set\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "# Compute the classification report for the test set\n",
    "report_test = classification_report(y_test, y_pred_test)\n",
    "\n",
    "# Print the classification report for the test set\n",
    "print(\"\\nTest report:\\n\", report_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " forward feature selection using decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected feature #1: Median (F1-score: 0.781)\n",
      "Selected feature #2: direction_pct (F1-score: 0.641)\n"
     ]
    }
   ],
   "source": [
    "# Create a list to store the selected features\n",
    "selected_features = []\n",
    "\n",
    "# Loop through the features and select the best one\n",
    "for i in range(len(X.columns)):\n",
    "    best_feature = None\n",
    "    best_score = 0\n",
    "    \n",
    "    for feature in X.columns:\n",
    "        # Skip the features that have already been selected\n",
    "        if feature in selected_features:\n",
    "            continue\n",
    "        \n",
    "        # Add the feature to the list of selected features\n",
    "        candidate_features = selected_features + [feature]\n",
    "        \n",
    "        # Train a decision tree model on the selected features\n",
    "        clf = DecisionTreeClassifier(random_state=42)\n",
    "        clf.fit(X_train[candidate_features], y_train)\n",
    "        \n",
    "        # Predict the class labels for the test set\n",
    "        y_pred = clf.predict(X_test[candidate_features])\n",
    "        \n",
    "        # Compute the F1-score for the test set\n",
    "        score = f1_score(y_test, y_pred, average='weighted')\n",
    "        \n",
    "        # Update the best feature if necessary\n",
    "        if score > best_score:\n",
    "            best_feature = feature\n",
    "            best_score = score\n",
    "    \n",
    "    # Add the best feature to the list of selected features\n",
    "    selected_features.append(best_feature)\n",
    "    print(f'Selected feature #{i+1}: {best_feature} (F1-score: {best_score:.3f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
